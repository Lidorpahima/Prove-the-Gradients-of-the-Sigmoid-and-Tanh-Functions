<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Step-by-step mathematical proofs for the gradients of activation functions: Sigmoid and Hyperbolic Tangent.">
    
</head>
<body>
    <h1>Activation Function Gradients</h1>
    <p>
        This repository contains detailed step-by-step mathematical proofs for the gradients of two common activation functions:
    </p>
    <ul>
        <li>
            <strong>Sigmoid function</strong>: Proves that 
            <code>dσ(x)/dx = σ(x)(1 - σ(x))</code>.
        </li>
        <li>
            <strong>Hyperbolic tangent function</strong>: Proves that 
            <code>d(tanh(x))/dx = 1 - tanh²(x)</code>.
        </li>
    </ul>
    <p>
        These proofs are essential for understanding the backpropagation process in neural networks.
    </p>
</body>
</html>
